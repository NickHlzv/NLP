{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from telegram import Update\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import annoy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "DATA_PATH = 'chatbot_files/'\n",
    "\n",
    "tqdm.pandas()  # enable 'progress_apply' function\n",
    "\n",
    "tbeg = time.perf_counter()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Предобработаем ответы mail.ru из файла: к каждому вопросу присоединим 1 ответ и запишем в файл на будущее.\n",
    "# Это позволит нам сэкономить время и ресурсы при дальнейшем препроцессинге текста\n",
    "\n",
    "question = None\n",
    "written = False\n",
    "\n",
    "# Мы идем по всем записям, берем первую строку как вопрос\n",
    "# и после знака --- находим ответ\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"w\", \"utf_8_sig\") as fout:\n",
    "    with open(\"Otvety.txt\", \"r\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            if line.startswith(\"---\"):\n",
    "                written = False\n",
    "                continue\n",
    "            if not written and question is not None:\n",
    "                fout.write(question.replace(\"\\t\", \" \").strip() + \"\\t\" + line.replace(\"\\t\", \" \"))\n",
    "                written = True\n",
    "                question = None\n",
    "                continue\n",
    "            if not written:\n",
    "                question = line.strip()\n",
    "                continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Так как строки иногда разрываются (по символу \\n) не после последнего поля), то приходится\n",
    "# преобразовать файл 'ProductsDataset.csv' так, чтобы строка заканчивалась после последовательности \".jpg\"\n",
    "# В итоге получаем новый файл 'prodDS.csv'.\n",
    "\n",
    "\n",
    "out_line = ''\n",
    "c = 0\n",
    "with codecs.open(DATA_PATH + \"prodDS.csv\", \"w\", \"utf_8_sig\") as fout:\n",
    "    with codecs.open(\"ProductsDataset.csv\", \"r\", \"utf_8_sig\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            c += 1\n",
    "            # print(c)\n",
    "            # if c>10:\n",
    "            #   break\n",
    "            if c > 1:\n",
    "                if line.endswith(\".jpg\\n\") != True:\n",
    "                    # print('No')\n",
    "                    # print(line)\n",
    "                    not_EOL = True\n",
    "                    out_line = (out_line+line).replace('\\n', ' ')\n",
    "                    continue\n",
    "                else:\n",
    "                    # print('Yes')\n",
    "                    out_line = out_line+line\n",
    "                    # print(out_line)\n",
    "                    fout.write(out_line)\n",
    "                    not_EOL = False\n",
    "                    out_line = ''\n",
    "            else:\n",
    "                fout.write(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"prodDS.csv\")\n",
    "df = df.fillna('')\n",
    "df['label'] = 0  # Добавим разметку\n",
    "df.columns = ['title', 'description', 'product_id', 'category_id', 'subcategory_id',\n",
    "              'properties', 'image_links', 'label']  # исправим 'descrirption' -> 'description'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Объединим названия и описания в один текстовый столбец так, как:\n",
    "# \"...продуктовым можно считать запрос, который равен названию или описанию товара\" .\n",
    "\n",
    "df1 = df[['description', 'label']]\n",
    "df2 = df[['title', 'label']]\n",
    "df1.columns = ['text', 'label']\n",
    "df2.columns = ['text', 'label']\n",
    "\n",
    "df_products = pd.concat([df1, df2], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Соберем датафрейм с вопросами из болталки\n",
    "questions = []\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"r\", \"utf_8_sig\") as f:\n",
    "    for line in tqdm(f):\n",
    "        spls = line.split(\"\\t\")\n",
    "        questions.append(spls[0])\n",
    "\n",
    "df_questions = pd.DataFrame({'text': questions})\n",
    "df_questions['label'] = 1  # Добавим разметку"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Собираем итоговый датасет\n",
    "table = pd.concat([df_products, df_questions], ignore_index=True)\n",
    "# Remove rows with empty text cells\n",
    "table.drop(index=table[table.text == ''].index, inplace=True)\n",
    "table.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "r = RussianStemmer()\n",
    "\n",
    "sw = set(get_stop_words(\"ru\")).union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "def pprocess(line):\n",
    "    line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "    line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "    line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [r.stem(i) for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return ' '.join(spls)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "table_processed = table['text'].progress_apply(lambda x: pprocess(x))\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = table_processed.to_list()\n",
    "y = table['label'].to_list()\n",
    "\n",
    "plt.hist(y)\n",
    "plt.show()\n",
    "s1 = (table['label'] == 1).sum()\n",
    "s0 = (table['label'] == 0).sum()\n",
    "print(f'Product({s0})/Non-product({s1}) = {s0/s1}')\n",
    "# Классы распределены неравномерно - будем использовать stratify при разбиении на тест и трейн"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make datasets with similar classes distribution\n",
    "TX_train, TX_test, ly_train, ly_test, text_train, text_test = train_test_split(\n",
    "    X, y, table['text'], test_size=0.20, random_state=42, shuffle=True, stratify=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "vectorizer.fit(TX_train)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "\n",
    "X_train = vectorizer.transform(TX_train)\n",
    "y_train = np.array(ly_train)\n",
    "\n",
    "X_test = vectorizer.transform(TX_test)\n",
    "y_test = np.array(ly_test)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create LinearSVM model object and fit the model\n",
    "model_svm = svm.LinearSVC()\n",
    "model_svm.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Сохраняем классификатор\n",
    "with open(DATA_PATH + 'query_model_stem_tdfv_best.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svm, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Посмотрим на различные рекомендуемые метрики при дисбалансе классов\n",
    "predictions = model_svm.predict(X_test)\n",
    "f1_s = f1_score(y_test, predictions)\n",
    "f2 = fbeta_score(y_test, predictions, beta=2.0)\n",
    "accuracy = (predictions == y_test).mean()\n",
    "print(accuracy, f1_s, f2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = y_test\n",
    "y_pred = predictions\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Всего запросов в тестовой выборке:', len(y_test))\n",
    "print('Правильно классифицированных продуктовых запросов:',\n",
    "      cm[0, 0], 'Ошибочно потерянных продуктовых запросов:', cm[0, 1])\n",
    "print('Ошибочно принятых за продуктовые запросы:',\n",
    "      cm[1, 0], 'Правильно классифицированных мусорных запросов:', cm[1, 1])\n",
    "print('Процент потерянных истинных продуктовых запросов:',\n",
    "      100*cm[0, 1]/cm[0, 0])\n",
    "print('Процент принятых мусорных запросов:', 100*cm[1, 0]/cm[1, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\")).union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def products_preprocess_txt(line):\n",
    "    line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "    line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "    line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Для обучения Word2Vec создадим датасет из названий и описаний длиной больше 20 символов.\n",
    "df_products = pd.concat(\n",
    "    [df1.loc[df1.text.str.len() > 20], df2], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "products_processed = df_products['text'].progress_apply(\n",
    "    lambda x: products_preprocess_txt(x))\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Обучаем модель для продуктовых запросов\n",
    "t = time.perf_counter()\n",
    "model_w2v_products = Word2Vec(\n",
    "    sentences=products_processed.to_list(), vector_size=100, min_count=1, window=5)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Складываем в индекс все наши названия продуктовых запросов\n",
    "t = time.perf_counter()\n",
    "index = annoy.AnnoyIndex(100, 'angular')\n",
    "\n",
    "index_map = {}\n",
    "counter = 0\n",
    "\n",
    "\n",
    "def annoy_indexing(row):\n",
    "    global counter\n",
    "    n_w2v = 0\n",
    "    index_map[counter] = row['product_id'] + ' ' + row['title']\n",
    "    product_query = products_preprocess_txt(row['title'])\n",
    "    vector = np.zeros(100)\n",
    "    for word in product_query:\n",
    "        if word in model_w2v_products.wv:\n",
    "            vector += model_w2v_products.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    index.add_item(counter, vector)\n",
    "    counter += 1\n",
    "\n",
    "df.apply(lambda x: annoy_indexing(x), 1)\n",
    "index.build(10)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_description(product_title):\n",
    "    preprocessed_title = products_preprocess_txt(product_title)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_title:\n",
    "        if word in model_w2v_products.wv:\n",
    "            vector += model_w2v_products.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index.get_nns_by_vector(vector, 1)\n",
    "    return index_map[answer_index[0]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "find_description('Юбка детская ORBY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "\n",
    "def preprocess_txt(line):\n",
    "    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "    spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "    return spls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.perf_counter()\n",
    "sentences = []\n",
    "c = 0\n",
    "with codecs.open(\"Otvety.txt\", \"r\", \"utf_8_sig\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        spls = preprocess_txt(line)\n",
    "        sentences.append(spls)\n",
    "        c += 1\n",
    "        if c > 500000:\n",
    "            break\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = [i for i in sentences if len(i) > 2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Обучим модель word2vec на наших вопросах\n",
    "t = time.perf_counter()\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, min_count=1, window=5)\n",
    "\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Складываем в индекс все наши вопросы из болталки\n",
    "t = time.perf_counter()\n",
    "index_s = annoy.AnnoyIndex(100, 'angular')\n",
    "\n",
    "index_map_s = {}\n",
    "counter = 0\n",
    "\n",
    "with codecs.open(DATA_PATH + \"prepared_answers.txt\", \"r\", \"utf_8_sig\") as f:\n",
    "\n",
    "    for line in tqdm(f):\n",
    "        n_w2v = 0\n",
    "        spls = line.split(\"\\t\")\n",
    "        if len(spls) > 1:\n",
    "            index_map_s[counter] = spls[1]\n",
    "            question = preprocess_txt(spls[0])\n",
    "            vector = np.zeros(100)\n",
    "            for word in question:\n",
    "                if word in model.wv:\n",
    "                    vector += model.wv[word]\n",
    "                    n_w2v += 1\n",
    "            if n_w2v > 0:\n",
    "                vector = vector / n_w2v\n",
    "            index_s.add_item(counter, vector)\n",
    "\n",
    "        counter += 1\n",
    "index_s.build(10)\n",
    "print('Elapsed, s:', time.perf_counter() - t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_answer(question):\n",
    "    preprocessed_question = preprocess_txt(question)\n",
    "    n_w2v = 0\n",
    "    vector = np.zeros(100)\n",
    "    for word in preprocessed_question:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            n_w2v += 1\n",
    "    if n_w2v > 0:\n",
    "        vector = vector / n_w2v\n",
    "    answer_index = index_s.get_nns_by_vector(vector, 1)\n",
    "    return index_map_s[answer_index[0]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "find_answer('Эбу в двенашке называется Итэлма что за эбу?')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Total Run Time Elapsed, s:', time.perf_counter() - tbeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    # Подгружаем ранее сохраненный и обученный векторайзер\n",
    "    with open(DATA_PATH + 'TfidfVectorizer.pickle', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    # Подгружаем ранее сохраненный и обученный классификатор\n",
    "    with open(DATA_PATH + 'query_model_stem_tdfv_best.pickle', 'rb') as f:\n",
    "        model_svm = pickle.load(f)\n",
    "\n",
    "    # Подгружаем предварительно обученную модель продуктовых названий\n",
    "    model_w2v_products = Word2Vec.load(DATA_PATH + \"w2v_model_products\")\n",
    "\n",
    "    # Подгружаем предварительно подготовленный и сохранненый индекс продуктовых названий\n",
    "    with open(DATA_PATH + 'index_map_products.pickle', 'rb') as f:\n",
    "        index_map = pickle.load(f)\n",
    "    index = annoy.AnnoyIndex(100, 'angular')\n",
    "    index.load(DATA_PATH + 'products.ann')\n",
    "\n",
    "    # Подгружаем предварительно обученную модель вопросов из болталки\n",
    "    model = Word2Vec.load(DATA_PATH + \"w2v_model\")\n",
    "\n",
    "    # Подгружаем предварительно подготовленный и сохранненый индекс ответов из болталки\n",
    "    with open(DATA_PATH + 'index_map_speaker.pickle', 'rb') as f:\n",
    "        index_map_s = pickle.load(f)\n",
    "    index_s = annoy.AnnoyIndex(100, 'angular')\n",
    "    index_s.load(DATA_PATH + 'speaker.ann')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "r = RussianStemmer()\n",
    "morpher = MorphAnalyzer()\n",
    "sw_ = set(get_stop_words(\"ru\"))\n",
    "sw = sw_.union(\n",
    "    set('1234567890qwertyuiopasdfghjklzxcvbnmйцукенгшщзхъфываепнроглшщджэячсмитьбю'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def get_answer(text):\n",
    "\n",
    "    def pprocess(line):\n",
    "        # используется для обработки текстовых данных при обучении классификатора\n",
    "        line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "        line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "        line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [r.stem(i) for i in spls]\n",
    "        spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "        return ' '.join(spls)\n",
    "\n",
    "    def products_preprocess_txt(line):\n",
    "        # используется для обработки текстовых данных при сворачивании продуктовых названий в  Word2Vec\n",
    "        line = re.sub(\"[\\x0b\\x0c\\s]\", ' ', line.lower())\n",
    "        line = re.sub(\"(б\\.у)|(б\\/у)|(б\\\\у)\", 'бу', line)\n",
    "        line = re.sub(\"[^а-яё \\d\\w]\", ' ', line)\n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "        spls = [i for i in spls if i not in sw and i != \"\"]\n",
    "        return spls\n",
    "\n",
    "    def preprocess_txt(line):\n",
    "        # используется для обработки текстовых данных при сворачивании вопросов из болталки в  Word2Vec\n",
    "        spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n",
    "        spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n",
    "        spls = [i for i in spls if i not in sw_ and i != \"\"]\n",
    "        return spls\n",
    "\n",
    "    def find_description(product_title):\n",
    "        # находит id и title входному продуктовому запросу\n",
    "        preprocessed_title = products_preprocess_txt(product_title)\n",
    "        n_w2v = 0\n",
    "        vector = np.zeros(100)\n",
    "        for word in preprocessed_title:\n",
    "            if word in model_w2v_products.wv:\n",
    "                vector += model_w2v_products.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        answer_index = index.get_nns_by_vector(vector, 1)\n",
    "        return index_map[answer_index[0]]\n",
    "\n",
    "    def find_answer(question):\n",
    "        # находит ответ из болталки по  входному вопросу\n",
    "        preprocessed_question = preprocess_txt(question)\n",
    "        n_w2v = 0\n",
    "        vector = np.zeros(100)\n",
    "        for word in preprocessed_question:\n",
    "            if word in model.wv:\n",
    "                vector += model.wv[word]\n",
    "                n_w2v += 1\n",
    "        if n_w2v > 0:\n",
    "            vector = vector / n_w2v\n",
    "        answer_index = index_s.get_nns_by_vector(vector, 1)\n",
    "        return index_map_s[answer_index[0]]\n",
    "\n",
    "\n",
    "\n",
    "    # product or speaker?\n",
    "    if model_svm.predict(vectorizer.transform([pprocess(text)]))[0] == 1:\n",
    "        # speaker\n",
    "        return find_answer(text)\n",
    "    else:\n",
    "        # product\n",
    "        return find_description(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def start(update: Update, context: CallbackContext):\n",
    "    update.message.reply_text('Привет! Назови свой вопрос')\n",
    "\n",
    "def text_msg(update: Update, context: CallbackContext):\n",
    "    txt_q = update.message.text # текст который пришёл\n",
    "    update.message.reply_text(get_answer(txt_q))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "updater = Updater(\"5987399402:AAFBjEvfZaGBcCpN3Csruh99Gr368kTP5E8\", use_context=True) # Token\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "# on different commands - answer in Telegram\n",
    "dispatcher.add_handler(CommandHandler(\"start\", start))\n",
    "dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, text_msg))\n",
    "\n",
    "# Start the Bot\n",
    "updater.start_polling()\n",
    "updater.idle()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
